{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65296955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importo las librerías necesarias\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32a5c581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando archivo original...\n",
      "Datos cargados. Se encontraron 37667 filas.\n"
     ]
    }
   ],
   "source": [
    "#Cargo el dataset de lesiones de 1951-2023\n",
    "print(f\"Cargando archivo original...\")\n",
    "try:\n",
    "    df_hasta_2023 = pd.read_csv(\"C:\\\\Users\\\\FranciscoJH\\\\Downloads\\\\NBA Player Injury Stats(1951 - 2023).csv\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: No se encontró el archivo 'C:\\\\Users\\\\FranciscoJH\\\\Downloads\\\\NBA Player Injury Stats(1951 - 2023).csv'. Asegúrate de que esté en la misma carpeta.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Datos cargados. Se encontraron {len(df_hasta_2023)} filas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7c68c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convirtiendo la columna 'Date' a formato de fecha...\n"
     ]
    }
   ],
   "source": [
    "# Conversión de Fecha\n",
    "# Nos aseguramos de que la columna 'Date' sea de tipo fecha\n",
    "columna_fecha = 'Date'\n",
    "# 'errors=coerce' convertirá cualquier fecha rota en 'NaT' (Not a Time)\n",
    "print(f\"Convirtiendo la columna '{columna_fecha}' a formato de fecha...\")\n",
    "df_hasta_2023[columna_fecha] = pd.to_datetime(df_hasta_2023[columna_fecha], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3224b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos filas donde la fecha no se pudo leer\n",
    "df_hasta_2023.dropna(subset=[columna_fecha], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "889e6300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtrando datos entre 2010 y 2023...\n"
     ]
    }
   ],
   "source": [
    "# Extraemos el año de la fecha y filtramos\n",
    "print(\"Filtrando datos entre 2010 y 2023...\")\n",
    "df_filtrado_hasta_2023 = df_hasta_2023[(df_hasta_2023[columna_fecha].dt.year >= 2010) & (df_hasta_2023[columna_fecha].dt.year <= 2023)]\n",
    "#Además elimino la primer columna que almacena el ID del registro, ya que el dataset de lesiones de 2016-2025 no la tiene\n",
    "df_filtrado_hasta_2023 = df_filtrado_hasta_2023.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce5daaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtrado completo. Se encontraron 22998 filas.\n"
     ]
    }
   ],
   "source": [
    "#Guardo el dataset filtrado\n",
    "print(f\"Filtrado completo. Se encontraron {len(df_filtrado_hasta_2023)} filas.\")\n",
    "df_filtrado_hasta_2023.to_csv(\"C:\\\\Users\\\\FranciscoJH\\\\Downloads\\\\NBA_Player_Injury_Stats_Filtrado_2023.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa5db650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando archivo original...\n",
      "Datos cargados. Se encontraron 16873 filas.\n"
     ]
    }
   ],
   "source": [
    "#Ahora cargo el dataset de lesiones de 2016-2025\n",
    "print(f\"Cargando archivo original...\")\n",
    "try:\n",
    "    df_hasta_2025 = pd.read_csv(\"C:\\\\Users\\\\FranciscoJH\\\\Downloads\\\\injury_data.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: No se encontró el archivo 'C:\\\\Users\\\\FranciscoJH\\\\Downloads\\\\injury_data.csv'. Asegúrate de que esté en la misma carpeta.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Datos cargados. Se encontraron {len(df_hasta_2025)} filas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b220cfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convirtiendo la columna 'Date' a formato de fecha...\n"
     ]
    }
   ],
   "source": [
    "# Conversión de Fecha\n",
    "# Nos aseguramos de que la columna 'Date' sea de tipo fecha\n",
    "columna_fecha = 'Date'\n",
    "# 'errors=coerce' convertirá cualquier fecha rota en 'NaT' (Not a Time)\n",
    "print(f\"Convirtiendo la columna '{columna_fecha}' a formato de fecha...\")\n",
    "df_hasta_2025[columna_fecha] = pd.to_datetime(df_hasta_2025[columna_fecha], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "181e6943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos filas donde la fecha no se pudo leer\n",
    "df_hasta_2025.dropna(subset=[columna_fecha], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "472d066f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtrando datos entre 2023 y 2025...\n"
     ]
    }
   ],
   "source": [
    "# Extraemos el año de la fecha y filtramos\n",
    "print(\"Filtrando datos entre 2023 y 2025...\")\n",
    "df_filtrado_hasta_2025 = df_hasta_2025[(df_hasta_2025[columna_fecha].dt.year >= 2023) & (df_hasta_2025[columna_fecha].dt.year <= 2025)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "118f0f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtrado completo. Se encontraron 3955 filas.\n"
     ]
    }
   ],
   "source": [
    "#Guardo el dataset filtrado\n",
    "print(f\"Filtrado completo. Se encontraron {len(df_filtrado_hasta_2025)} filas.\")\n",
    "df_filtrado_hasta_2025.to_csv(\"C:\\\\Users\\\\FranciscoJH\\\\Downloads\\\\NBA_Player_Injury_Stats_Filtrado_2025.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d1d08f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! Archivo final guardado en 'C:\\Users\\FranciscoJH\\Downloads\\NBA_Player_Injury_Stats_2010_2025.csv'\n"
     ]
    }
   ],
   "source": [
    "#Ahora juntos los dos datasets\n",
    "df_total = pd.concat([df_filtrado_hasta_2023, df_filtrado_hasta_2025], ignore_index=True)\n",
    "\n",
    "#Elijo las columnas clave para eliminar los duplicados de año 2023\n",
    "columnas_clave = ['Date', 'Team', 'Acquired', 'Relinquished', 'Notes']\n",
    "\n",
    "#Elimino duplicados basándome en las columnas clave\n",
    "#'keep=' controla cuál duplicado se queda:\n",
    "#'last':  Se queda con el último registro de los duplicados que ve.\n",
    "lesiones_csv = df_total.drop_duplicates(subset=columnas_clave, keep='last')\n",
    "\n",
    "#Guardo el dataset final\n",
    "lesiones_csv.to_csv(\"C:\\\\Users\\\\FranciscoJH\\\\Downloads\\\\NBA_Player_Injury_Stats_2010_2025.csv\", index=False)\n",
    "print(f\"¡Éxito! Archivo final guardado en 'C:\\\\Users\\\\FranciscoJH\\\\Downloads\\\\NBA_Player_Injury_Stats_2010_2025.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47500db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna 'player_name_temp' creada en df_lesiones.\n",
      "Merge completado.\n",
      "Porcentaje de nulos en 'player_id': 10.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FranciscoJH\\AppData\\Local\\Temp\\ipykernel_7900\\4113195975.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  lesiones_csv['player_name_temp'] = lesiones_csv['Acquired'].fillna(lesiones_csv['Relinquished'])\n",
      "C:\\Users\\FranciscoJH\\AppData\\Local\\Temp\\ipykernel_7900\\4113195975.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  lesiones_csv['player_name_temp'] = lesiones_csv['player_name_temp'].str.strip('• ')\n"
     ]
    }
   ],
   "source": [
    "#El dataset no tiene el ID del jugador lesionado, algo que es necesario para relacionarlo con otros datasets.\n",
    "#Por lo tanto ahora voy a agregar el ID del jugador utilizando el dataset de jugadores de la NBA\n",
    "# --- 0. Cargo el dataset de jugadores ---\n",
    "df_player = pd.read_csv('C:\\\\Users\\\\FranciscoJH\\\\Downloads\\\\player.csv')\n",
    "\n",
    "\n",
    "# --- 1. Unificar Nombres en df_lesiones ---\n",
    "# Creamos una columna temporal 'player_name'\n",
    "# Usamos .fillna() para \"rellenar\" los nulos de 'Acquired' con los valores de 'Relinquished'\n",
    "# Esto nos da una sola columna con el nombre del jugador, sin importar la acción.\n",
    "lesiones_csv['player_name_temp'] = lesiones_csv['Acquired'].fillna(lesiones_csv['Relinquished'])\n",
    "\n",
    "# Quitamos los '•', por si hay alguno\n",
    "lesiones_csv['player_name_temp'] = lesiones_csv['player_name_temp'].str.strip('• ')\n",
    "\n",
    "print(f\"Columna 'player_name_temp' creada en df_lesiones.\")\n",
    "\n",
    "\n",
    "# --- 2. Preparar df_player ---\n",
    "# Seleccionamos solo las columnas que necesitamos para la fusión\n",
    "# (Esto evita duplicar columnas como 'first_name', 'last_name', etc.)\n",
    "df_player_mapa = df_player[['id', 'full_name']]\n",
    "\n",
    "\n",
    "# --- 3. Fusionar (Merge) ---\n",
    "# Unimos df_lesiones con df_player_mapa usando nuestras columnas de nombres\n",
    "# how='left' asegura que mantengamos TODAS las lesiones, \n",
    "# incluso si un jugador no se encuentra en df_player (quedará como NaN)\n",
    "df_lesiones_con_id = pd.merge(\n",
    "    lesiones_csv,\n",
    "    df_player_mapa,\n",
    "    left_on='player_name_temp', # La columna unificada de df_lesiones\n",
    "    right_on='full_name',     # La columna de nombre de df_player\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"Merge completado.\")\n",
    "\n",
    "# --- 4. Limpiar el DataFrame Final ---\n",
    "# Renombramos la columna 'id' (que vino de df_player) a 'player_id'\n",
    "df_lesiones_con_id.rename(columns={'id': 'player_id'}, inplace=True)\n",
    "\n",
    "# Eliminamos las columnas temporales/duplicadas que ya no necesitamos\n",
    "df_lesiones_con_id = df_lesiones_con_id.drop(columns=['player_name_temp', 'full_name'])\n",
    "\n",
    "#Verifico el porcentaje de nulos en player_id\n",
    "total_filas = len(df_lesiones_con_id)\n",
    "nulos_player_id = df_lesiones_con_id['player_id'].isnull().sum()\n",
    "porcentaje_nulos = (nulos_player_id / total_filas) * 100\n",
    "\n",
    "print(f\"Porcentaje de nulos en 'player_id': {porcentaje_nulos:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50806e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Investigando Nombres que Fallaron el Merge (Versión Corregida) ---\n",
      "Se encontraron 232 nombres únicos que no tuvieron coincidencia.\n",
      "\n",
      "--- Top 20 Nombres Fallidos (y cuántas veces aparecen) ---\n",
      "Acquired\n",
      "(William) Tony Parker                            80\n",
      "Mike Conley Jr.                                  67\n",
      "Emanuel Ginobili / Manu Ginobili                 66\n",
      "James Michael McAdoo / James McAdoo (Michael)    65\n",
      "Wesley Matthews / Wes Matthews Jr.               63\n",
      "Marcus Morris                                    59\n",
      "Maximilian Kleber / Maxi Kleber                  55\n",
      "John Wall (Hildred)                              54\n",
      "Ogugua Anunoby / O.G. Anunoby                    43\n",
      "Nene / Nene Hilario / Maybyner Hilario           43\n",
      "Metta World Peace / Ron Artest                   42\n",
      "Raulzinho Neto / Raul Neto                       40\n",
      "Louis Williams / Lou Williams                    39\n",
      "Maurice Harkless / Moe Harkless                  38\n",
      "C.J. Miles                                       38\n",
      "Byron Mullens / B.J. Mullens                     36\n",
      "John Collins (Martin)                            35\n",
      "Nicolas Claxton / Nic Claxton                    33\n",
      "(James) Mike Scott                               33\n",
      "Amare Stoudemire / Amar'e Stoudemire             32\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Como vemos, hay un porcentaje de nulos del 10.71% en la columna player_id.\n",
    "#Esto puede deberse a que algunos nombres en el dataset de lesiones no coinciden exactamente con los del dataset de jugadores,\n",
    "#por ejemplo, diferencias en apodos, errores tipográficos, o formatos distintos (como incluir iniciales, sufijos, etc.).\n",
    "#Para mejorar la coincidencia, podríamos implementar técnicas de limpieza de datos más avanzadas.\n",
    "#Pero primero, veremos que nombres no tienen un ID asignado.\n",
    "\n",
    "print(\"--- Investigando Nombres que Fallaron el Merge (Versión Corregida) ---\")\n",
    "\n",
    "# 1. Filtramos las filas donde player_id es nulo\n",
    "df_fallos = df_lesiones_con_id[df_lesiones_con_id['player_id'].isnull()]\n",
    "\n",
    "# --- 2. LA CORRECCIÓN ---\n",
    "# Como 'player_name_temp' ya no existe, recreamos la lógica\n",
    "# usando las columnas originales ('Acquired' y 'Relinquished')\n",
    "# que SÍ están en 'df_fallos'.\n",
    "\n",
    "# Rellenamos los nulos de 'Acquired' con los de 'Relinquished'\n",
    "nombres_unificados_fallidos = df_fallos['Acquired'].fillna(df_fallos['Relinquished'])\n",
    "\n",
    "# Limpiamos (el '•' y los espacios, por si hay algunos)\n",
    "nombres_unificados_fallidos = nombres_unificados_fallidos.str.strip('• ')\n",
    "\n",
    "\n",
    "# 3. Obtenemos la lista de nombres ÚNICOS que fallaron\n",
    "nombres_fallidos = nombres_unificados_fallidos.value_counts()\n",
    "\n",
    "print(f\"Se encontraron {len(nombres_fallidos)} nombres únicos que no tuvieron coincidencia.\")\n",
    "print(\"\\n--- Top 20 Nombres Fallidos (y cuántas veces aparecen) ---\")\n",
    "print(nombres_fallidos.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c14c6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna 'player_name_temp' creada.\n",
      "Mapa de jugadores (para búsqueda) creado.\n",
      "Obteniendo nombres únicos de 'lesiones'...\n",
      "Se encontraron 1564 nombres únicos para buscar (vs 26019 filas totales).\n",
      "Creando mapa de traducción... (Esto puede tardar unos minutos, pero solo una vez)\n",
      "Mapa de traducción creado. Se encontraron 1466 coincidencias.\n",
      "Aplicando mapa de traducción a todo el DataFrame...\n",
      "¡Merge difuso completado!\n",
      "Porcentaje de nulos en 'player_id' (después del fuzzy merge): 1.40%\n"
     ]
    }
   ],
   "source": [
    "#Ahora voy a utilizar otra librería para hacer un matching más avanzado entre los nombres.\n",
    "#La librería se llama 'fuzzywuzzy' y permite comparar cadenas de texto con técnicas de coincidencia difusa.\n",
    "from fuzzywuzzy import process, fuzz \n",
    "\n",
    "# --- 1. Unificar Nombres en df_lesiones ---\n",
    "df_lesiones_con_id['player_name_temp'] = df_lesiones_con_id['Acquired'].fillna(df_lesiones_con_id['Relinquished'])\n",
    "df_lesiones_con_id['player_name_temp'] = df_lesiones_con_id['player_name_temp'].str.strip('• ')\n",
    "print(\"Columna 'player_name_temp' creada.\")\n",
    "\n",
    "# --- 2. Preparar el \"Mapa de Búsqueda\" de df_player ---\n",
    "# Convertimos a lista para que fuzzywuzzy trabaje más rápido\n",
    "nombres_player = df_player['full_name'].tolist() \n",
    "mapa_ids_player = dict(zip(df_player['full_name'], df_player['id']))\n",
    "print(\"Mapa de jugadores (para búsqueda) creado.\")\n",
    "\n",
    "# --- 3. La Función de Búsqueda ---\n",
    "def encontrar_mejor_coincidencia(nombre_lesion, lista_nombres_player, mapa_ids):\n",
    "    if pd.isna(nombre_lesion):\n",
    "        return None\n",
    "    try:\n",
    "        # Usamos 'fuzz.token_set_ratio' que es bueno para nombres (ignora \"Jr.\", \"II\", etc.)\n",
    "        mejor_coincidencia, score = process.extractOne(\n",
    "            nombre_lesion, \n",
    "            lista_nombres_player, \n",
    "            scorer=fuzz.token_set_ratio, # Usamos un 'scorer' robusto\n",
    "            score_cutoff=85 # Umbral de confianza del 85%\n",
    "        )\n",
    "        \n",
    "        if score >= 85:\n",
    "            return mapa_ids[mejor_coincidencia]\n",
    "        else:\n",
    "            return None # No estaba lo suficientemente seguro\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# --- 4. Crear el Mapa de Traducción usando .unique() ---\n",
    "print(\"Obteniendo nombres únicos de 'lesiones'...\")\n",
    "# Obtenemos la lista corta de nombres únicos a buscar\n",
    "nombres_unicos_lesiones = df_lesiones_con_id['player_name_temp'].dropna().unique()\n",
    "\n",
    "print(f\"Se encontraron {len(nombres_unicos_lesiones)} nombres únicos para buscar (vs {len(df_lesiones_con_id)} filas totales).\")\n",
    "print(\"Creando mapa de traducción... (Esto puede tardar unos minutos, pero solo una vez)\")\n",
    "\n",
    "# Creamos el diccionario de traducción\n",
    "traduccion_mapa = {}\n",
    "for nombre_lesion in nombres_unicos_lesiones:\n",
    "    id_encontrado = encontrar_mejor_coincidencia(nombre_lesion, nombres_player, mapa_ids_player)\n",
    "    if id_encontrado is not None:\n",
    "        traduccion_mapa[nombre_lesion] = id_encontrado\n",
    "\n",
    "print(f\"Mapa de traducción creado. Se encontraron {len(traduccion_mapa)} coincidencias.\")\n",
    "\n",
    "# --- 5. Aplicar el Mapa ---\n",
    "print(\"Aplicando mapa de traducción a todo el DataFrame...\")\n",
    "# .map() es órdenes de magnitud más rápido que .apply()\n",
    "df_lesiones_con_id['player_id'] = df_lesiones_con_id['player_name_temp'].map(traduccion_mapa)\n",
    "\n",
    "print(\"¡Merge difuso completado!\")\n",
    "\n",
    "# --- 6. Verificación ---\n",
    "total_filas = len(df_lesiones_con_id)\n",
    "nulos_player_id = df_lesiones_con_id['player_id'].isnull().sum()\n",
    "porcentaje_nulos = (nulos_player_id / total_filas) * 100\n",
    "\n",
    "print(f\"Porcentaje de nulos en 'player_id' (después del fuzzy merge): {porcentaje_nulos:.2f}%\")\n",
    "\n",
    "# Eliminamos las columnas temporales/duplicadas que ya no necesitamos\n",
    "df_lesiones_con_id = df_lesiones_con_id.drop(columns=['player_name_temp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb3e539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas antes de la limpieza final: 26019\n",
      "Filas después de la limpieza final: 25654\n",
      "Se eliminaron 365 filas (el 1.40%).\n"
     ]
    }
   ],
   "source": [
    "#Como vemos que el porcentaje de nulos es bajo, por eso decidí eliminar esas filas, ya que si no tienen ID no podré relacionarlas con otros datasets.\n",
    "\n",
    "filas_antes = len(df_lesiones_con_id)\n",
    "print(f\"Filas antes de la limpieza final: {filas_antes}\")\n",
    "\n",
    "# .dropna() elimina las filas donde la columna especificada ('player_id') es Nula\n",
    "df_limpio_final = df_lesiones_con_id.dropna(subset=['player_id'])\n",
    "\n",
    "filas_despues = len(df_limpio_final)\n",
    "print(f\"Filas después de la limpieza final: {filas_despues}\")\n",
    "print(f\"Se eliminaron {filas_antes - filas_despues} filas (el 1.40%).\")\n",
    "\n",
    "# --- GUARDAR ESTE ARCHIVO ---\n",
    "df_limpio_final.to_csv(\"C:\\\\Users\\\\FranciscoJH\\\\Desktop\\\\lesiones_listas.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
